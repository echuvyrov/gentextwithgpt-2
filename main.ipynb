{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c134327-ebca-4854-b878-7978edb7cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3afa4c4-35d2-41bb-a4b4-65901ac753a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "!{sys.executable} -m pip install transformers\n",
    "!{sys.executable} -m pip install textstat\n",
    "!{sys.executable} -m pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56748fcb-3a26-4ee4-b0dd-1b3ad9924e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08454f8-ff6a-4ebe-9f69-4953589e2814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb09f9ca-21e5-4ef0-9164-dfc0e42ef309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee754be3-c3c7-4a05-863e-dfc843336848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb4a05a-a33f-428d-a176-8041b8e2700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4376f24a-66c4-4df5-81b1-f0381e9891e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e636d5-92ca-435c-8cfc-9635ac07681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ea2b9-5f19-4e1a-8dfd-eb0747eb5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import textstat\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class Util:\n",
    "\n",
    "    def stem(self, word_to_stem: str):\n",
    "        ps = PorterStemmer()\n",
    "        return ps.stem(word_to_stem)\n",
    "\n",
    "class Book:\n",
    "    book_title = \"\"\n",
    "    book_text = []\n",
    "\n",
    "    def tikify(self, level: str):\n",
    "        return \"tikified\"\n",
    "\n",
    "class FileIO:\n",
    "\n",
    "    def load_level_vocab(self, reading_level: int):\n",
    "        utils = Util()\n",
    "        level_appropriate_words = {}\n",
    "\n",
    "        # level-appropriate words include all levels UP to the one passed in\n",
    "        for i in range(0, reading_level + 1):\n",
    "            words_file = open(os.path.abspath(\"data/\" + \"words_\" + str(i) + \".txt\"))\n",
    "            for line in words_file:\n",
    "                if len(line) > 1 and not line.startswith(\"@\"):\n",
    "                    # skip processing empty and copyright lines\n",
    "                    words_trimmed_spaces = line.replace(\"  \", \" \")\n",
    "                    words = words_trimmed_spaces.split(\" \")\n",
    "                \n",
    "                    for word in words:\n",
    "                        level_appropriate_words[utils.stem(word)] = utils.stem(word)\n",
    "\n",
    "        return level_appropriate_words\n",
    "\n",
    "    def load_cbt_train_data(self):\n",
    "        all_books = {}\n",
    "        new_book = Book()\n",
    "\n",
    "        cbt_file = open(os.path.abspath(\"data/cbt_train.txt\"))\n",
    "        for line in cbt_file:\n",
    "            if line.startswith(\"_BOOK_TITLE_\"):\n",
    "                # skip the very beginning of the file - no book data yet\n",
    "                if len(new_book.book_title) > 0:\n",
    "                    all_books[new_book.book_title] = Book()\n",
    "                    all_books[new_book.book_title].book_title = new_book.book_title\n",
    "                    all_books[new_book.book_title].book_text = new_book.book_text\n",
    "                    \n",
    "                # start a new book\n",
    "                new_book_title = line.replace(\"_BOOK_TITLE_ : \", \"\")\n",
    "                new_book.book_title = new_book_title.rstrip()\n",
    "                new_book.book_text = []\n",
    "\n",
    "            else:\n",
    "                #keep appending to book\n",
    "                new_book.book_text.append(line)\n",
    "\n",
    "        return all_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00371c11-d071-4f0d-99c2-a2fc399b8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = Util()\n",
    "file_io = FileIO()\n",
    "\n",
    "def __init__(desired_reading_level):\n",
    "    # mainly kick off the pipeline using the command-line args passed in\n",
    "    # desired_reading_level = sys.argv[1]\n",
    "    kick_off_text_gen_pipeline(int(desired_reading_level))\n",
    "\n",
    "\n",
    "def kick_off_text_gen_pipeline(desired_reading_level):\n",
    "    # load training data\n",
    "    train_data = file_io.load_cbt_train_data()\n",
    "    # baseline it to the desired level\n",
    "    baseline(train_data, desired_reading_level)\n",
    "\n",
    "\n",
    "def baseline(train_data: dict, reading_level: int):\n",
    "\n",
    "    k_lvl = file_io.load_level_vocab(reading_level)\n",
    "\n",
    "    book_title = \"Lewis_Carroll___Alice's_Adventures_Under_Ground.txt.out\"\n",
    "    # unaltered training text\n",
    "    print(\"BOOK TITLE xxx'\" + str(book_title) + \"'xxx\")\n",
    "    #print(\"BOOK TEXT \" + str(train_data[book_title].book_text))\n",
    "    print(\"Total length \" + str(sum(len(word) for word in train_data[book_title].book_text)))\n",
    "\n",
    "    # remove all words but the k-level (stemmed) and kept all the capitalized words as named entities\n",
    "    leveled_book_text = []\n",
    "    for sentence in train_data[book_title].book_text:\n",
    "        original_sentence_readability_score = textstat.flesch_kincaid_grade(sentence)\n",
    "        print(\"ORIGINAL sentence \" + str(sentence))\n",
    "        print(\"ORIGINAL sentence readability score \" + str(original_sentence_readability_score))\n",
    "        leveled_sentence = \"\"\n",
    "        for word in sentence.split():\n",
    "            if utils.stem(word) in k_lvl or word[0].isupper():\n",
    "                 leveled_sentence += word + \" \"\n",
    "        leveled_book_text.append(leveled_sentence)\n",
    "        leveled_sentence_readability_score = textstat.flesch_kincaid_grade(leveled_sentence)\n",
    "        print(\"LEVELED sentence \" + str(leveled_sentence))\n",
    "        print(\"LEVELED sentence readability score \" + str(leveled_sentence_readability_score))\n",
    "\n",
    "    train_data[book_title].book_text = leveled_book_text\n",
    "    print(\"LEVELED BOOK TITLE xxx'\" + str(book_title) + \"'xxx\")\n",
    "    #print(\"LEVELED BOOK TEXT \" + str(train_data[book_title].book_text))\n",
    "    print(\"LEVELED Total length \" + str(sum(len(word) for word in train_data[book_title].book_text)))\n",
    "\n",
    "def baseline_sentence_list(train_data: list, reading_level: int):\n",
    "\n",
    "    k_lvl = file_io.load_level_vocab(reading_level)\n",
    "\n",
    "    # remove all words but the k-level (stemmed) and kept all the capitalized words as named entities\n",
    "    leveled_book_text = []\n",
    "    for sentence in train_data:\n",
    "        original_sentence_readability_score = textstat.flesch_kincaid_grade(sentence)\n",
    "        leveled_sentence = \"\"\n",
    "        for word in sentence.split():\n",
    "            if utils.stem(word) in k_lvl or word[0].isupper():\n",
    "                 leveled_sentence += word + \" \"\n",
    "        leveled_book_text.append(leveled_sentence)\n",
    "        leveled_sentence_readability_score = textstat.flesch_kincaid_grade(leveled_sentence)\n",
    "\n",
    "    return leveled_book_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fda08405-061e-4322-bf17-5648c70298ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load text of all books\n",
    "train_data = file_io.load_cbt_train_data()\n",
    "# get just Lucy Maud Montgomery's short stories since she has the most sentences with accessible readability for kids\n",
    "book_title = \"Lucy_Maud_Montgomery___Lucy_Maud_Montgomery_Short_Stories,_1909_to_1922.txt.out\"\n",
    "lucy_maud_sentences = train_data[book_title].book_text\n",
    "\n",
    "# plot sentence lengths\n",
    "sentence_lengths = []\n",
    "grade_level_3_sentences = []\n",
    "\n",
    "for sentence in lucy_maud_sentences:\n",
    "    readability_score = textstat.flesch_kincaid_grade(sentence)\n",
    "    if readability_score < 3:\n",
    "        grade_level_3_sentences.append(sentence)\n",
    "        # get rough token count distribution\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        sentence_lengths.append(len(tokens))\n",
    "\n",
    "sentence_lengths = np.array(sentence_lengths)\n",
    "sns.histplot(data=sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f26e7532-5ae0-42a6-a7ed-23b8abca38a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.314572864321608"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139eb8d-64b6-4412-b9c4-fe4dce7b003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GPT-2 Text Generation Code that follows was heavily borrowed from https://colab.research.google.com/drive/13dZVYEOMhXhkXWfvSMVM1TTtUDrT6Aeh?usp=sharing#scrollTo=x0WeP5PREUuy\n",
    "# Load the GPT tokenizer.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc6dc5e-160d-4d7e-8c42-c8d45d10005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The max model length is {} for this model, although the actual embedding size for GPT small is 768\".format(tokenizer.model_max_length))\n",
    "print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
    "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
    "print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd2a66d-ba6d-4f9b-96c3-1d6ad6256b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "class GPT2Dataset(Dataset):\n",
    "\n",
    "  def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "    self.input_ids = []\n",
    "    self.attn_masks = []\n",
    "\n",
    "    for txt in txt_list:\n",
    "      encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.attn_masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a435d0-aa9c-462a-8a63-44c747b53197",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GPT2Dataset(grade_level_3_sentences, tokenizer, max_length=768)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc4c51-28b7-458a-ab5b-4543010f1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoaders for our training and validation datasets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bff15dd-f386-4fc3-b2da-fa75d2980a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not really doing anything special with the config\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "\n",
    "# instantiate the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "\n",
    "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device = torch.device(\"cuda\")\n",
    "model.cuda()\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# some parameters I cooked up that work reasonably well\n",
    "\n",
    "epochs = 2\n",
    "learning_rate = 5e-4\n",
    "warmup_steps = 1e2\n",
    "epsilon = 1e-8\n",
    "\n",
    "# this produces sample output every 100 steps\n",
    "sample_every = 100\n",
    "\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate,\n",
    "                  eps = epsilon\n",
    "                )\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "# This changes the learning rate as the training loop progresses\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = warmup_steps, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b2363-2240-4ff6-b239-7e136fb78854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
    "\n",
    "total_t0 = time.time()\n",
    "training_stats = []\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "        model.zero_grad()        \n",
    "        outputs = model(  b_input_ids,\n",
    "                          labels=b_labels, \n",
    "                          attention_mask = b_masks,\n",
    "                          token_type_ids=None\n",
    "                        )\n",
    "\n",
    "        loss = outputs[0]  \n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "\n",
    "        # Get sample every x batches.\n",
    "        if step % sample_every == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
    "            model.eval()\n",
    "\n",
    "            sample_outputs = model.generate(\n",
    "                                    bos_token_id=random.randint(1,30000),\n",
    "                                    do_sample=True,   \n",
    "                                    top_k=50, \n",
    "                                    max_length = 200,\n",
    "                                    top_p=0.95, \n",
    "                                    num_return_sequences=1\n",
    "                                )\n",
    "            for i, sample_output in enumerate(sample_outputs):\n",
    "                  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "            model.train()\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)       \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs  = model(b_input_ids, \n",
    "#                            token_type_ids=None, \n",
    "                             attention_mask = b_masks,\n",
    "                            labels=b_labels)\n",
    "          \n",
    "            loss = outputs[0]  \n",
    "            \n",
    "        batch_loss = loss.item()\n",
    "        total_eval_loss += batch_loss        \n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    validation_time = format_time(time.time() - t0)    \n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc602d4-ece7-4353-9750-fb4cfadf7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "output_dir = './model_lucy_maud_level_13_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89820911-cf1a-4aeb-9d9b-1f200ffbfa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "prompt = \"<|startoftext|>\"\n",
    "\n",
    "# encode context the generation is conditioned on\n",
    "input_ids = torch.tensor(tokenizer.encode('<|startoftext|>Baby Shark')).unsqueeze(0)\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 300\n",
    "greedy_output = model.generate(input_ids, max_length=300, do_sample=True, top_k=0, num_return_sequences=5)\n",
    "for i, greedy_output in enumerate(greedy_output):\n",
    "  print(\"Greedy: {}: {}\\n\\n\".format(i, tokenizer.decode(greedy_output, skip_special_tokens=True)))\n",
    "\n",
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate( input_ids, max_length=500, num_beams=1, do_sample=True, top_k=0, num_return_sequences=5)\n",
    "for i, beam_output in enumerate(beam_output):\n",
    "    print(\"Beam: {}: {}\\n\\n\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccca0e0-dfa7-4ee5-acf5-1da110508059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTS with Lucy Maud Montgomery\n",
    "# Load text, load models and baseline\n",
    "# get just Lucy Maud Montgomery's short stories since she has the most sentences with accessible readability for kids\n",
    "alice_in_wonderland_book = \"Lucy_Maud_Montgomery___Lucy_Maud_Montgomery_Short_Stories,_1909_to_1922.txt.out\"\n",
    "alice_in_wonderland_sentences = train_data[alice_in_wonderland_book].book_text[0:10]\n",
    "\n",
    "print(\"Original Text: First 10 sentences\")\n",
    "print (' '.join(alice_in_wonderland_sentences).replace('\\n', ' '))\n",
    "\n",
    "# Readability test works on individual sentences, so average that out\n",
    "avg_reading_lvl = sum([textstat.flesch_kincaid_grade(x) for x in alice_in_wonderland_sentences])/len(alice_in_wonderland_sentences)\n",
    "print(\"\\nEstimated Average Reading Level (Flesch Kincaid grade) \" + str(avg_reading_lvl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d0539c-5a67-48f1-8d1f-becf0a4943d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline Text for Level K (Kindergarten): First 10 sentences\")\n",
    "alice_in_wonderland_baseline_3 = baseline_sentence_list(alice_in_wonderland_sentences, 0)\n",
    "print (' '.join(alice_in_wonderland_baseline_3).replace('\\n', ' '))\n",
    "\n",
    "# Readability test works on individual sentences, so average that out\n",
    "avg_reading_lvl = sum([textstat.flesch_kincaid_grade(x) for x in alice_in_wonderland_baseline_3])/len(alice_in_wonderland_baseline_3)\n",
    "print(\"\\nEstimated Average Reading Level (Flesch Kincaid grade) \" + str(avg_reading_lvl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa04e7-49c9-4820-bb9f-57fe3cdf2c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generated Text for Level K (Kindergarten): First 10 sentences\")\n",
    "\n",
    "# Load the pre-saved model\n",
    "#configuration3 = GPT2Config.from_pretrained('./model_lucy_maud_level3_save/', output_hidden_states=False)\n",
    "#model3 = GPT2LMHeadModel.from_pretrained('./model_lucy_maud_level3_save/', config=configuration)\n",
    "#model3 = model3.to(device)\n",
    "\n",
    "generated_for_k_level = []\n",
    "# for each sentence, take the first 3 words and generate the rest, appropriate for the level\n",
    "for sentence in alice_in_wonderland_sentences:\n",
    "    first_3_words = ' '.join(sentence.split()[:3])\n",
    "    tokenized_first_3_words = torch.tensor(tokenizer.encode('<|startoftext|>' + first_3_words)).unsqueeze(0)\n",
    "    tokenized_first_3_words = tokenized_first_3_words.to(device)\n",
    "    greedy_alice = model.generate(tokenized_first_3_words, max_length=300, do_sample=True, top_k=0, num_return_sequences=1)\n",
    "    generated_for_k_level.append((tokenizer.decode(greedy_alice[0], skip_special_tokens=True)).replace(\"\\n\", \" \"))\n",
    "\n",
    "print (' '.join(generated_for_k_level))\n",
    "\n",
    "# Readability test works on individual sentences, so average that out\n",
    "avg_reading_lvl = sum([textstat.flesch_kincaid_grade(x) for x in generated_for_k_level])/len(generated_for_k_level)\n",
    "print(\"\\nEstimated Average Reading Level (Flesch Kincaid grade) \" + str(avg_reading_lvl))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94397955-ee52-4353-810c-8fb0e36ce771",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generated Text for Level 13 (High School): First 10 sentences\")\n",
    "\n",
    "# Load pre-saved model\n",
    "configuration13 = GPT2Config.from_pretrained('./model_lucy_maud_level_13_save/', output_hidden_states=False)\n",
    "model13 = GPT2LMHeadModel.from_pretrained('./model_lucy_maud_level_13_save/', config=configuration)\n",
    "model13 = model13.to(device)\n",
    "\n",
    "generated_for_k_level = []\n",
    "# for each sentence, take the first 3 words and generate the rest, appropriate for the level\n",
    "for sentence in alice_in_wonderland_sentences:\n",
    "    first_3_words = ' '.join(sentence.split()[:3])\n",
    "    tokenized_first_3_words = torch.tensor(tokenizer.encode('<|startoftext|>' + first_3_words)).unsqueeze(0)\n",
    "    tokenized_first_3_words = tokenized_first_3_words.to(device)\n",
    "    greedy_alice = model.generate(tokenized_first_3_words, max_length=300, do_sample=True, top_k=0, num_return_sequences=1)\n",
    "    generated_for_k_level.append((tokenizer.decode(greedy_alice[0], skip_special_tokens=True)).replace(\"\\n\", \" \"))\n",
    "\n",
    "print (' '.join(generated_for_k_level))\n",
    "\n",
    "# Readability test works on individual sentences, so average that out\n",
    "avg_reading_lvl = sum([textstat.flesch_kincaid_grade(x) for x in generated_for_k_level])/len(generated_for_k_level)\n",
    "print(\"\\nEstimated Average Reading Level (Flesch Kincaid grade) \" + str(avg_reading_lvl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6724ab7-8a98-4a86-a2e3-5b2ad59a2225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
